import torch
from torch import nn
from torch.nn import functional as F

class AE(nn.Module):
	def __init__(self, **kwargs):
		super(AE, self).__init__()
		for key in kwargs:
			setattr(self, key, kwargs[key])

		self.activation = getattr(nn, kwargs['activation'])()
		self.input_dim = self.num_joints * self.joint_dims * self.window_size
		
		self.enc_sizes = [self.input_dim] + self.hidden_sizes
		enc_layers = []
		for i in range(len(self.enc_sizes)-1):
			enc_layers.append(nn.Linear(self.enc_sizes[i], self.enc_sizes[i+1]))
			enc_layers.append(self.activation)
		self._encoder = nn.Sequential(*enc_layers)

		self.latent = nn.Linear(self.enc_sizes[-1], self.latent_dim)
		
		self.dec_sizes = [self.latent_dim] + self.hidden_sizes[::-1]
		dec_layers = []
		for i in range(len(self.dec_sizes)-1):
			dec_layers.append(nn.Linear(self.dec_sizes[i], self.dec_sizes[i+1]))
			dec_layers.append(self.activation)
		self._decoder = nn.Sequential(*dec_layers)
		self._output = nn.Linear(self.dec_sizes[-1], self.input_dim) 

	def forward(self, x, encode_only=False):
		enc = self._encoder(x)
		z_samples = self.latent(enc)
		if encode_only:
			return z_samples
		x_gen = self._output(self._decoder(z_samples))
		return x_gen, z_samples, None
	
	def latent_loss(self, zpost_samples, zpost_dist):
		return 0
