{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Latent dimesnions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (161x35 and 20x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/vignesh/playground/hsmmvae/latent_plotter.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvp_dlws/home/vignesh/playground/hsmmvae/latent_plotter.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \tx_r \u001b[39m=\u001b[39m x[:, model_h\u001b[39m.\u001b[39minput_dim:]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvp_dlws/home/vignesh/playground/hsmmvae/latent_plotter.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \tz_h \u001b[39m=\u001b[39m model_h(x_h, encode_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvp_dlws/home/vignesh/playground/hsmmvae/latent_plotter.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \tz_r \u001b[39m=\u001b[39m model_r(x_r, encode_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvp_dlws/home/vignesh/playground/hsmmvae/latent_plotter.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \tz_encoded\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mconcat([z_h, z_r], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()) \u001b[39m# (num_trajs, seq_len, 2*z_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvp_dlws/home/vignesh/playground/hsmmvae/latent_plotter.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m hsmm_np \u001b[39m=\u001b[39m pbd\u001b[39m.\u001b[39mHMM(nb_dim\u001b[39m=\u001b[39margs_ckpt\u001b[39m.\u001b[39mlatent_dim\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, nb_states\u001b[39m=\u001b[39margs_ckpt\u001b[39m.\u001b[39mhsmm_components)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/playground/hsmmvae/vae.py:42\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, encode_only, dist_only)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, encode_only \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, dist_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 42\u001b[0m \tenc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder(x)\n\u001b[1;32m     43\u001b[0m \tz_mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_mean(enc)\n\u001b[1;32m     44\u001b[0m \t\u001b[39mif\u001b[39;00m encode_only:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (161x35 and 20x40)"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from vae import *\n",
    "import dataloaders\n",
    "\n",
    "import pbdlib as pbd\n",
    "import pbdlib_torch as pbd_torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ckpt_path = 'logs/2023aug/bp_pepper_downsampled_robotfuture/z03h05/diagvaehmmgmm/models/init_ckpt.pth'\n",
    "ckpt = torch.load(ckpt_path)\n",
    "hyperparams = np.load(os.path.join(os.path.dirname(ckpt_path),'hyperparams.npz'), allow_pickle=True)\n",
    "args_ckpt = hyperparams['args'].item()\n",
    "ae_config = hyperparams['ae_config'].item()\n",
    "if 'robot_vae_config' in hyperparams.keys():\n",
    "\trobot_vae_config = hyperparams['robot_vae_config'].item()\n",
    "else:\n",
    "\trobot_vae_config = hyperparams['ae_config'].item()\n",
    "\n",
    "model_h = VAE(**(ae_config.__dict__)).to(device)\n",
    "model_h.load_state_dict(ckpt['model_h'])\n",
    "model_h.eval()\n",
    "model_r = VAE(**(robot_vae_config.__dict__)).to(device)\n",
    "model_r.load_state_dict(ckpt['model_r'])\n",
    "model_r.eval()\n",
    "z_dim = args_ckpt.latent_dim\n",
    "\n",
    "if args_ckpt.dataset == 'buetepage_pepper':\n",
    "\tdataset = dataloaders.buetepage.PepperWindowDataset\n",
    "elif args_ckpt.dataset == 'buetepage':\n",
    "\tdataset = dataloaders.buetepage.HHWindowDataset\n",
    "# TODO: Nuitrack\n",
    "\n",
    "train_iterator = DataLoader(dataset(args_ckpt.src, train=True, window_length=args_ckpt.window_size, downsample=args_ckpt.downsample), batch_size=1, shuffle=True)\n",
    "hsmm=[]\n",
    "with torch.no_grad():\n",
    "\tfor a in range(len(train_iterator.dataset.actidx)):\n",
    "\t\thsmm.append(pbd_torch.HMM(nb_dim=args_ckpt.latent_dim*2, nb_states=args_ckpt.hsmm_components))\n",
    "\t\ts = train_iterator.dataset.actidx[a]\n",
    "\t\tz_encoded = []\n",
    "\t\tfor j in range(s[0], s[1]):\n",
    "\t\t\t# for j in np.random.randint(s[0], s[1], 12):\n",
    "\t\t\tx, label = train_iterator.dataset[j]\n",
    "\t\t\tx = torch.Tensor(x).to(device)\n",
    "\t\t\tx_h = x[:, :model_h.input_dim]\n",
    "\t\t\tx_r = x[:, model_h.input_dim:]\n",
    "\t\t\t\n",
    "\t\t\tz_h = model_h(x_h, encode_only=True)\n",
    "\t\t\tz_r = model_r(x_r, encode_only=True)\n",
    "\t\t\tz_encoded.append(torch.concat([z_h, z_r], dim=-1).cpu().numpy()) # (num_trajs, seq_len, 2*z_dim)\n",
    "\t\thsmm_np = pbd.HMM(nb_dim=args_ckpt.latent_dim*2, nb_states=args_ckpt.hsmm_components)\n",
    "\t\thsmm_np.init_params_scikit(np.concatenate(z_encoded))\n",
    "\t\thsmm_np.em(z_encoded, reg=args_ckpt.cov_reg, reg_finish=args_ckpt.cov_reg)\n",
    "\t\thsmm[a].reg = torch.Tensor(hsmm_np.reg).to(device).requires_grad_(False)\n",
    "\t\thsmm[a].mu = torch.Tensor(hsmm_np.mu).to(device).requires_grad_(False)\n",
    "\t\thsmm[a].sigma = torch.Tensor(hsmm_np.sigma).to(device).requires_grad_(False)\n",
    "\t\thsmm[a].priors = torch.Tensor(hsmm_np.priors).to(device).requires_grad_(False)\n",
    "\t\thsmm[a].trans = torch.Tensor(hsmm_np.trans).to(device).requires_grad_(False)\n",
    "\t\thsmm[a].Trans = torch.Tensor(hsmm_np.Trans).to(device).requires_grad_(False)\n",
    "\t\thsmm[a].init_priors = torch.Tensor(hsmm_np.init_priors).to(device).requires_grad_(False)\n",
    "\n",
    "# hsmm = ckpt['hsmm']\n",
    "\n",
    "test_dataset = dataset(args_ckpt.src, train=False, window_length=model_h.window_size, downsample=args_ckpt.downsample)\n",
    "actions = ['Hand Wave', 'Hand Shake', 'Rocket Fistbump', 'Parachute Fistbump']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax_dists = []\n",
    "ax_alpha= [] \n",
    "ax_trans = []\n",
    "for i in range(4):\n",
    "\tax_dists.append(fig.add_subplot(3, 4, i+1, projection='3d'))\n",
    "\tax_alpha.append(fig.add_subplot(3, 4, 4+i+1))\n",
    "\tax_trans.append(fig.add_subplot(3, 4, 8+i+1))\n",
    "actidx = np.hstack(test_dataset.actidx - np.array([0,1]))\n",
    "for a in actidx:\n",
    "\tx, label = test_dataset[a]\n",
    "\tseq_len = x.shape[0]\n",
    "\tdims_h = model_h.input_dim\n",
    "\tx = torch.Tensor(x).to(device)\n",
    "\tx_h = x[:, :dims_h]\n",
    "\tx_r = x[:, dims_h:]\n",
    "\t\n",
    "\tzh_post = model_h(x_h, dist_only=True)\n",
    "\tzr_post = model_r(x_r, dist_only=True)\n",
    "\tax_dists[label].scatter3D(zh_post.mean[::5, 0].detach().cpu().numpy(), zh_post.mean[::5, 1].detach().cpu().numpy(), zh_post.mean[::5, 2].detach().cpu().numpy(), 'r.')\n",
    "\tax_dists[label].scatter3D(zr_post.mean[::5, 0].detach().cpu().numpy(), zr_post.mean[::5, 1].detach().cpu().numpy(), zr_post.mean[::5, 2].detach().cpu().numpy(), 'b.')\n",
    "\n",
    "\talpha = hsmm[label].forward_variable(marginal=[], sample_size=100).detach().cpu().numpy()\n",
    "\talpha_h = hsmm[label].forward_variable(demo=zh_post.mean, marginal=slice(0, z_dim)).detach().cpu().numpy()\n",
    "\tax_alpha[label].plot(np.linspace(0, 1, 100), alpha.T, linestyle='-')\n",
    "\tax_alpha[label].plot(np.linspace(0, 1, seq_len), alpha_h.T, linestyle='--')\n",
    "\n",
    "\tax_trans[label].imshow(hsmm[label].Trans.detach().cpu().numpy())\n",
    "\t\n",
    "\tfor i in range(hsmm[label].nb_states):\n",
    "\t\tpbd.plot_gauss3d(ax_dists[label], hsmm[label].mu[i, :3].detach().cpu().numpy(), hsmm[label].sigma[i, :3, :3].detach().cpu().numpy(),\n",
    "\t\t\t\t\tcolor='red', alpha=0.1)\n",
    "\t\tpbd.plot_gauss3d(ax_dists[label], hsmm[label].mu[i, z_dim:z_dim+3].detach().cpu().numpy(), hsmm[label].sigma[i, z_dim:z_dim+3, z_dim:z_dim+3].detach().cpu().numpy(),\n",
    "\t\t\t\t\tcolor='blue', alpha=0.1)\n",
    "\t\t\n",
    "\t# break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
